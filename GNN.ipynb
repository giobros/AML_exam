{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "824fce14",
   "metadata": {},
   "source": [
    "GNN-based molecular property prediction\\\n",
    "A GNN is a neural network designed to operate on graph-structured data, andolecules are naturally represented as graphs where atoms are nodes and bonds are edges. \\\n",
    "Each node and edge can have features.\\\n",
    " The GNN learns by passing messages between neighboring nodes (atoms), gradually learning a graph-level representation (i.e., for the whole molecule).\\\n",
    "Use RDKit to convert SMILES into graphs:\\\n",
    "Nodes = atoms with features \\\n",
    "Edges = chemical bonds between atoms\\\n",
    "Graph label = whether molecule crosses blood-brain barrier\n",
    "\n",
    "Aim:\\\n",
    "Use the built-in MoleculeNet BBBP graph dataset\n",
    "Train a simple GCN-based GNN\n",
    "Print test accuracy\\\n",
    "\n",
    "GCN (Graph Convolutional Network).\\\n",
    "GCN sums over all neighbors of node, including the node itself and aggregate all information. It's the simplest GNN, using a mean-based aggregation.\n",
    "Basic vs Deep GCN: see what happens using more layers and wider hidden dimensions.\n",
    " \n",
    "\n",
    "GAT (Graph Attention Network):\n",
    "GAT uses attention mechanism to weight neighbor contributions\n",
    "Learns which neighbors are more important for each node\n",
    "More flexible than GCN but computationally heavier\n",
    "\n",
    " Graph Isomorphism Network (GIN):\n",
    " GIN has been proposed to overcome the disadvantages of GCNs. GIN is based on the Weisfeiler-Lehman graph isomorphism test. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fd8ca8",
   "metadata": {},
   "source": [
    "| Model          | Key Idea                             | Pros                               | Cons                      |\n",
    "| -------------- | ------------------------------------ | ---------------------------------- | ------------------------- |\n",
    "| **GCN\\_Basic** | Mean aggregation (2 layers)          | Fast, simple baseline              | Limited capacity          |\n",
    "| **GCN\\_Deep**  | Deeper GCN (4 layers)                | More expressive                    | Risk of over-smoothing    |\n",
    "| **GAT**        | Attention-weighted neighbors         | Learns importance of each neighbor | Slower, higher memory     |\n",
    "| **GIN**        | MLP-based, strong structure learning | High expressive power              | Complex, heavier training |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68af902c",
   "metadata": {},
   "source": [
    "Reference: https://projects.volkamerlab.org/teachopencadd/talktorials/T035_graph_neural_networks.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5072c96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BBBP dataset...\n",
      "Dataset size: 2039\n",
      "Number of node features: 9\n",
      "Number of edge features: 3\n",
      "Number of classes: 2\n",
      "\n",
      " Training GCN_Basic...\n",
      "  Epoch 50 | Train Loss: 0.3211 | Val AUC: 0.8838\n",
      "  Epoch 100 | Train Loss: 0.2904 | Val AUC: 0.8882\n",
      "  Epoch 150 | Train Loss: 0.2816 | Val AUC: 0.8880\n",
      "  Epoch 200 | Train Loss: 0.2902 | Val AUC: 0.8883\n",
      "  Epoch 250 | Train Loss: 0.2835 | Val AUC: 0.8875\n",
      "  Epoch 300 | Train Loss: 0.2964 | Val AUC: 0.8873\n",
      "   GCN_Basic Test ROC-AUC: 0.8991\n",
      "\n",
      " Training GCN_Deep...\n",
      "  Epoch 50 | Train Loss: 0.2923 | Val AUC: 0.8856\n",
      "  Epoch 100 | Train Loss: 0.2616 | Val AUC: 0.8846\n",
      "  Epoch 150 | Train Loss: 0.2598 | Val AUC: 0.8828\n",
      "  Epoch 200 | Train Loss: 0.2471 | Val AUC: 0.8832\n",
      "  Epoch 250 | Train Loss: 0.2611 | Val AUC: 0.8856\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import (\n",
    "    GCNConv, GATConv, GINConv,\n",
    "    global_mean_pool, global_max_pool, global_add_pool\n",
    ")\n",
    "\n",
    "# Load BBBP dataset\n",
    "print(\"Loading BBBP dataset...\")\n",
    "dataset = MoleculeNet(root='./data', name='BBBP')\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Number of node features: {dataset.num_node_features}\")\n",
    "print(f\"Number of edge features: {dataset.num_edge_features}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "# Dataset splits\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset = dataset[:train_size]\n",
    "val_dataset = dataset[train_size:train_size + val_size]\n",
    "test_dataset = dataset[train_size + val_size:]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# ==================== MODELS ====================\n",
    "\n",
    "# GCN / GAT model\n",
    "class EnhancedGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=3, dropout_rate=0.3, use_attention=False):\n",
    "        super(EnhancedGNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.use_attention = use_attention\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        \n",
    "        if use_attention:\n",
    "            self.convs.append(GATConv(input_dim, hidden_dim, heads=4, concat=False))\n",
    "        else:\n",
    "            self.convs.append(GCNConv(input_dim, hidden_dim))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "        \n",
    "        for _ in range(num_layers - 1):\n",
    "            if use_attention:\n",
    "                self.convs.append(GATConv(hidden_dim, hidden_dim, heads=4, concat=False))\n",
    "            else:\n",
    "                self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim * 3, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc_out = nn.Linear(hidden_dim // 2, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = x.float()\n",
    "        edge_index = edge_index.long()\n",
    "        \n",
    "        for i, (conv, bn) in enumerate(zip(self.convs, self.batch_norms)):\n",
    "            x_new = F.relu(bn(conv(x, edge_index)))\n",
    "            x_new = self.dropout(x_new)\n",
    "            if i > 0 and x.size(-1) == x_new.size(-1):\n",
    "                x = x + x_new\n",
    "            else:\n",
    "                x = x_new\n",
    "\n",
    "        x1 = global_mean_pool(x, batch)\n",
    "        x2 = global_max_pool(x, batch)\n",
    "        x3 = global_add_pool(x, batch)\n",
    "        x = torch.cat([x1, x2, x3], dim=1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc_out(x)\n",
    "\n",
    "# GIN Model\n",
    "class GINGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=3, dropout_rate=0.3):\n",
    "        super(GINGNN, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            mlp = nn.Sequential(\n",
    "                nn.Linear(input_dim if i == 0 else hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "            self.convs.append(GINConv(mlp))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 3, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc_out = nn.Linear(hidden_dim // 2, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = x.float()\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "            x = bn(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x1 = global_mean_pool(x, batch)\n",
    "        x2 = global_max_pool(x, batch)\n",
    "        x3 = global_add_pool(x, batch)\n",
    "        x = torch.cat([x1, x2, x3], dim=1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc_out(x)\n",
    "\n",
    "# ==================== TRAINING ====================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index, batch.batch)\n",
    "        loss = loss_fn(out, batch.y.view(-1, 1).float())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch.x, batch.edge_index, batch.batch).view(-1)\n",
    "            probs = torch.sigmoid(out).cpu().numpy()\n",
    "            labels = batch.y.cpu().numpy()\n",
    "            all_preds.extend(probs)\n",
    "            all_labels.extend(labels)\n",
    "    \n",
    "    binary_preds = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    return {\n",
    "        'f1': f1_score(all_labels, binary_preds),\n",
    "        'roc_auc': roc_auc_score(all_labels, all_preds),\n",
    "        'accuracy': accuracy_score(all_labels, binary_preds),\n",
    "        'precision': precision_score(all_labels, binary_preds),\n",
    "        'recall': recall_score(all_labels, binary_preds)\n",
    "    }, all_preds, all_labels\n",
    "\n",
    "# ==================== MODEL TRAINING ====================\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "models_to_test = {\n",
    "    'GCN_Basic': EnhancedGNN(input_dim=dataset.num_node_features, hidden_dim=64, num_layers=2, use_attention=False),\n",
    "    'GCN_Deep': EnhancedGNN(input_dim=dataset.num_node_features, hidden_dim=128, num_layers=4, use_attention=False),\n",
    "    'GAT': EnhancedGNN(input_dim=dataset.num_node_features, hidden_dim=128, num_layers=3, use_attention=True),\n",
    "    'GIN': GINGNN(input_dim=dataset.num_node_features, hidden_dim=128, num_layers=3)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models_to_test.items():\n",
    "    print(f\"\\n Training {model_name}...\")\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=20, factor=0.5)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_metrics_history = {'f1': [], 'roc_auc': [], 'accuracy': [], 'precision': [], 'recall': []}\n",
    "    best_val_auc = 0\n",
    "    patience_counter = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, 301):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "        train_losses.append(train_loss)\n",
    "        val_metrics, _, _ = evaluate_model(model, val_loader, device)\n",
    "\n",
    "        for metric_name, value in val_metrics.items():\n",
    "            val_metrics_history[metric_name].append(value)\n",
    "\n",
    "        scheduler.step(val_metrics['roc_auc'])\n",
    "\n",
    "        if val_metrics['roc_auc'] > best_val_auc:\n",
    "            best_val_auc = val_metrics['roc_auc']\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"  Epoch {epoch} | Train Loss: {train_loss:.4f} | Val AUC: {val_metrics['roc_auc']:.4f}\")\n",
    "\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    test_metrics, test_preds, test_labels = evaluate_model(model, test_loader, device)\n",
    "\n",
    "    results[model_name] = {\n",
    "        'test_metrics': test_metrics,\n",
    "        'val_history': val_metrics_history,\n",
    "        'train_losses': train_losses,\n",
    "        'training_time': (time.time() - start_time) / 60\n",
    "    }\n",
    "\n",
    "    print(f\"   {model_name} Test ROC-AUC: {test_metrics['roc_auc']:.4f}\")\n",
    "\n",
    "# ==================== PLOTTING ====================\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "for model_name in results:\n",
    "    plt.plot(results[model_name]['val_history']['roc_auc'], label=model_name)\n",
    "plt.title('ROC-AUC')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "for model_name in results:\n",
    "    plt.plot(results[model_name]['val_history']['f1'], label=model_name)\n",
    "plt.title('F1 Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "for model_name in results:\n",
    "    plt.plot(results[model_name]['train_losses'], label=model_name)\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "x = np.arange(len(results))\n",
    "width = 0.15\n",
    "metrics = ['f1', 'roc_auc', 'accuracy', 'precision', 'recall']\n",
    "for i, metric in enumerate(metrics):\n",
    "    vals = [results[k]['test_metrics'][metric] for k in results]\n",
    "    plt.bar(x + i*width, vals, width, label=metric.upper())\n",
    "plt.xticks(x + width*2, list(results.keys()), rotation=45)\n",
    "plt.title('Test Metrics')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "times = [results[k]['training_time'] for k in results]\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
    "plt.bar(list(results.keys()), times, color=colors)\n",
    "plt.title('Training Time (minutes)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.axis('off')\n",
    "txt = \"Architecture Summary:\\n\\n\"\n",
    "txt += \"- GCN_Basic: 2-layer GCN, 64 hidden\\n\"\n",
    "txt += \"- GCN_Deep: 4-layer GCN, 128 hidden\\n\"\n",
    "txt += \"- GAT: 3-layer GAT, 128 hidden, 4 heads\\n\"\n",
    "txt += \"- GIN: 3-layer GIN, 128 hidden\\n\"\n",
    "txt += \"\\nAll use multi-pooling and dropout\"\n",
    "plt.text(0.1, 0.9, txt, fontfamily='monospace', verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==================== FINAL RESULTS ====================\n",
    "print(\"\\nFINAL RESULTS SUMMARY\")\n",
    "for model_name in results:\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for k, v in results[model_name]['test_metrics'].items():\n",
    "        print(f\"  {k.capitalize():<10}: {v:.4f}\")\n",
    "    print(f\"  Training Time: {results[model_name]['training_time']:.2f} min\")\n",
    "\n",
    "best_model = max(results, key=lambda m: results[m]['test_metrics']['roc_auc'])\n",
    "print(f\"\\n Best Model: {best_model} (ROC-AUC: {results[best_model]['test_metrics']['roc_auc']:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML_exam_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
