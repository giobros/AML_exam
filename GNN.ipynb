{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "824fce14",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">GNN-based molecular property prediction\n",
    "\n",
    "A GNN is a neural network designed to operate on graph-structured data, andolecules are naturally represented as graphs where atoms are nodes and bonds are edges. \\\n",
    "Each node and edge can have features.\\\n",
    " The GNN learns by passing messages between neighboring nodes (atoms), gradually learning a graph-level representation (i.e., for the whole molecule).\\\n",
    "Use RDKit to convert SMILES into graphs:\\\n",
    "Nodes = atoms with features \\\n",
    "Edges = chemical bonds between atoms\\\n",
    "Graph label = whether molecule crosses blood-brain barrier\n",
    "\n",
    "Aim:\\\n",
    "Use the built-in MoleculeNet BBBP graph dataset\n",
    "Train a simple GCN-based GNN\n",
    "Print test accuracy\\\n",
    "\n",
    "GCN (Graph Convolutional Network).\\\n",
    "GCN sums over all neighbors of node, including the node itself and aggregate all information. It's the simplest GNN, using a mean-based aggregation.\n",
    "Basic vs Deep GCN: see what happens using more layers and wider hidden dimensions.\n",
    " \n",
    "\n",
    "GAT (Graph Attention Network):\n",
    "GAT uses attention mechanism to weight neighbor contributions\n",
    "Learns which neighbors are more important for each node\n",
    "More flexible than GCN but computationally heavier\n",
    "\n",
    " Graph Isomorphism Network (GIN):\n",
    " GIN has been proposed to overcome the disadvantages of GCNs. GIN is based on the Weisfeiler-Lehman graph isomorphism test. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fd8ca8",
   "metadata": {},
   "source": [
    "| Model          | Key Idea                             | Pros                               | Cons                      |\n",
    "| -------------- | ------------------------------------ | ---------------------------------- | ------------------------- |\n",
    "| **GCN\\_Basic** | Mean aggregation (2 layers)          | Fast, simple baseline              | Limited capacity          |\n",
    "| **GCN\\_Deep**  | Deeper GCN (4 layers)                | More expressive                    | Risk of over-smoothing    |\n",
    "| **GAT**        | Attention-weighted neighbors         | Learns importance of each neighbor | Slower, higher memory     |\n",
    "| **GIN**        | MLP-based, strong structure learning | High expressive power              | Complex, heavier training |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68af902c",
   "metadata": {},
   "source": [
    "Reference:\\\n",
    "paper doi: 10.1186/s13321-020-00479-8\\\n",
    "tutorial https://projects.volkamerlab.org/teachopencadd/talktorials/T035_graph_neural_networks.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "751498ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import (\n",
    "    GCNConv, GATConv, GINConv,\n",
    "    global_mean_pool, global_max_pool, global_add_pool\n",
    ")\n",
    "import random\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaa4919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of node features: 9\n",
      "Number of edge features: 3\n",
      "Number of classes: 2\n",
      "\n",
      " Training GCN_Basic...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# LOAD DATASET \n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "dataset = MoleculeNet(root='./data', name='BBBP')\n",
    "\n",
    "\n",
    "print(f\"Number of node features: {dataset.num_node_features}\")\n",
    "print(f\"Number of edge features: {dataset.num_edge_features}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset = dataset[:train_size]\n",
    "val_dataset = dataset[train_size:train_size + val_size]\n",
    "test_dataset = dataset[train_size + val_size:]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# MODELS \n",
    "class EnhancedGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=3, dropout_rate=0.3, use_attention=False):\n",
    "        super(EnhancedGNN, self).__init__()\n",
    "        self.use_attention = use_attention\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        if use_attention:\n",
    "            self.convs.append(GATConv(input_dim, hidden_dim, heads=4, concat=False))\n",
    "        else:\n",
    "            self.convs.append(GCNConv(input_dim, hidden_dim))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            if use_attention:\n",
    "                self.convs.append(GATConv(hidden_dim, hidden_dim, heads=4, concat=False))\n",
    "            else:\n",
    "                self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 3, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc_out = nn.Linear(hidden_dim // 2, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = x.float()\n",
    "        edge_index = edge_index.long()\n",
    "        for i, (conv, bn) in enumerate(zip(self.convs, self.batch_norms)):\n",
    "            x_new = F.relu(bn(conv(x, edge_index)))\n",
    "            x_new = self.dropout(x_new)\n",
    "            x = x + x_new if i > 0 and x.size(-1) == x_new.size(-1) else x_new\n",
    "        \n",
    "        # Multi-scale graph-level pooling (mean + max + sum)\n",
    "        x1 = global_mean_pool(x, batch)  # Average node features\n",
    "        x2 = global_max_pool(x, batch)   # Max node features  \n",
    "        x3 = global_add_pool(x, batch)   # Sum node features\n",
    "        x = torch.cat([x1, x2, x3], dim=1)  # Concatenate all pooling results\n",
    "        \n",
    "        # Final classification layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc_out(x)  # Raw logits for BCEWithLogitsLoss\n",
    "\n",
    "class GINGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=3, dropout_rate=0.3):\n",
    "        super(GINGNN, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            mlp = nn.Sequential(\n",
    "                nn.Linear(input_dim if i == 0 else hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "            self.convs.append(GINConv(mlp))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 3, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc_out = nn.Linear(hidden_dim // 2, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = x.float()\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "            x = bn(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x1 = global_mean_pool(x, batch)\n",
    "        x2 = global_max_pool(x, batch)\n",
    "        x3 = global_add_pool(x, batch)\n",
    "        x = torch.cat([x1, x2, x3], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc_out(x)\n",
    "\n",
    "# TRAINING FUNCTIONS \n",
    "def train_epoch(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index, batch.batch)\n",
    "        loss = loss_fn(out, batch.y.view(-1, 1).float())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch.x, batch.edge_index, batch.batch).view(-1)\n",
    "            probs = torch.sigmoid(out).cpu().numpy()\n",
    "            labels = batch.y.cpu().numpy()\n",
    "            all_preds.extend(probs)\n",
    "            all_labels.extend(labels)\n",
    "    binary_preds = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    return {\n",
    "        'f1': f1_score(all_labels, binary_preds),\n",
    "        'roc_auc': roc_auc_score(all_labels, all_preds),\n",
    "        'accuracy': accuracy_score(all_labels, binary_preds),\n",
    "        'precision': precision_score(all_labels, binary_preds),\n",
    "        'recall': recall_score(all_labels, binary_preds)\n",
    "    }, all_preds, all_labels\n",
    "\n",
    "# MODEL TRAINING WITH EARLY STOPPING \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "models_to_test = {\n",
    "    'GCN_Basic': EnhancedGNN(input_dim=dataset.num_node_features, hidden_dim=64, num_layers=2, use_attention=False),\n",
    "    'GCN_Deep': EnhancedGNN(input_dim=dataset.num_node_features, hidden_dim=128, num_layers=4, use_attention=False),\n",
    "    'GAT': EnhancedGNN(input_dim=dataset.num_node_features, hidden_dim=128, num_layers=3, use_attention=True),\n",
    "    'GIN': GINGNN(input_dim=dataset.num_node_features, hidden_dim=128, num_layers=3)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models_to_test.items():\n",
    "    print(f\"\\n Training {model_name}...\")\n",
    "    model = model.to(device)\n",
    "    # Using Adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=20, factor=0.5)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_metrics_history = {'f1': [], 'roc_auc': [], 'accuracy': [], 'precision': [], 'recall': []}\n",
    "    best_val_auc = 0\n",
    "    patience_counter = 0\n",
    "    patience_limit = 30\n",
    "    best_epoch = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, 302):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "        train_losses.append(train_loss)\n",
    "        val_metrics, _, _ = evaluate_model(model, val_loader, device)\n",
    "\n",
    "        for metric_name, value in val_metrics.items():\n",
    "            val_metrics_history[metric_name].append(value)\n",
    "\n",
    "        scheduler.step(val_metrics['roc_auc'])\n",
    "\n",
    "        if val_metrics['roc_auc'] > best_val_auc:\n",
    "            best_val_auc = val_metrics['roc_auc']\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            best_epoch = epoch\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience_limit:\n",
    "            print(f\"   Early stopping at epoch {epoch} (Best epoch: {best_epoch})\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    test_metrics, test_preds, test_labels = evaluate_model(model, test_loader, device)\n",
    "\n",
    "    results[model_name] = {\n",
    "        'test_metrics': test_metrics,\n",
    "        'val_history': val_metrics_history,\n",
    "        'train_losses': train_losses,\n",
    "        'training_time': (time.time() - start_time) / 60\n",
    "    }\n",
    "\n",
    "    print(f\"   {model_name} Test ROC-AUC: {test_metrics['roc_auc']:.4f} (Best epoch: {best_epoch})\")\n",
    "\n",
    "# PLOTTING \n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "for model_name in results:\n",
    "    plt.plot(results[model_name]['val_history']['roc_auc'], label=model_name)\n",
    "plt.title('ROC-AUC')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "for model_name in results:\n",
    "    plt.plot(results[model_name]['val_history']['f1'], label=model_name)\n",
    "plt.title('F1 Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "for model_name in results:\n",
    "    plt.plot(results[model_name]['train_losses'], label=model_name)\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "x = np.arange(len(results))\n",
    "width = 0.15\n",
    "metrics = ['f1', 'roc_auc', 'precision','accuracy', 'recall']\n",
    "for i, metric in enumerate(metrics):\n",
    "    vals = [results[k]['test_metrics'][metric] for k in results]\n",
    "    plt.bar(x + i*width, vals, width, label=metric.upper())\n",
    "plt.xticks(x + width*2, list(results.keys()), rotation=45)\n",
    "plt.title('Test Metrics')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "times = [results[k]['training_time'] for k in results]\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
    "plt.bar(list(results.keys()), times, color=colors)\n",
    "plt.title('Training Time (minutes)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.axis('off')\n",
    "txt = \"Architecture Summary:\\n\\n\"\n",
    "txt += \"- GCN_Basic: 2-layer GCN, 64 hidden\\n\"\n",
    "txt += \"- GCN_Deep: 4-layer GCN, 128 hidden\\n\"\n",
    "txt += \"- GAT: 3-layer GAT, 128 hidden, 4 heads\\n\"\n",
    "txt += \"- GIN: 3-layer GIN, 128 hidden\\n\"\n",
    "txt += \"\\nAll use multi-pooling (mean + max + sum) and dropout\"\n",
    "plt.text(0.1, 0.9, txt, fontfamily='monospace', verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# FINAL RESULTS \n",
    "print(\"\\nFINAL RESULTS SUMMARY\")\n",
    "for model_name in results:\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for k, v in results[model_name]['test_metrics'].items():\n",
    "        print(f\"  {k.capitalize():<10}: {v:.4f}\")\n",
    "    print(f\"  Training Time: {results[model_name]['training_time']:.2f} min\")\n",
    "\n",
    "best_model = max(results, key=lambda m: results[m]['test_metrics']['roc_auc'])\n",
    "print(f\"\\nBest Model: {best_model} (ROC-AUC: {results[best_model]['test_metrics']['roc_auc']:.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffd0bca",
   "metadata": {},
   "source": [
    "<small><i>The ROC AUC scores for all the models are very close and good.\\\n",
    "This means all of them perform well in distinguishing between the two classes.\\\n",
    "Here I used Early Stopping (based on validation ROC-AUC) to stop the training when parameter updates no longer begin to yield improves on a validation set.\n",
    "\n",
    "<small><i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeceabe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML_exam_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
